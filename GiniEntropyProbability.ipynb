{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor. @LuisFalva\n",
    "## La impureza de Gini es una magnitud de incertidumbre, esto se refiere a la aleatoriedad con la que se elige un _elemento$_{i}$_ del conjunto de datos y la probabilidad de asignarle al _elemento$_{i}$_ una etiqueta _m_ de forma incorrecta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $Gini = 1 - \\sum_{i=1}^{c} (p_{i})^{2}$\n",
    "\n",
    "## Análisis de la función.\n",
    "### Para cada clase se deberá calcular la probabilidad que le corresponda, esta medida se le considera la probabilidad que posee cada clase de ser escogida. $p_{i}$ es la probabilidad calculada para cada clase, las probabilidades se pueden calcular por separado. Simplificando, podemos decir que crearemos primero (y por separado) una función que calcule las $p_{i}$'s y así poder usarla almacenando en un diccionario de python los valores de las $k$ clases.\n",
    "\n",
    "### Supongamos que partimos la función y almacenamos en una nueva variable $P = \\sum (p_{i})^2$ la suma las probabilidades al cuadrado. Ahora bien, si se aplica $1 - P$ esto representa el _complemento_ de la probabilidad $P$ i.e. la parte _contraria de la probabilidad_.\n",
    "\n",
    "### Por lo tanto $1 - P$ la consideramos una maginitud contraria a la suma de las probabilidades al cuadrado $P$, esa magnitud describe la probabilidad de ser etiquetado de forma incorrecta por el nivel de impureza en los datos, de aquí nace la idea de la _magnitud de incertidumbre_, i.e. _Gini_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por ahora crearemos nuestros nodos los cuales almacenarán información de cada clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Root_node = {\"Manzana\": 100, \"Platano\": 400, \"Mango\": 5, \"Uva\": 550}\n",
    "child_node_st = {\"Manzana\": 100, \"Platano\": 400, \"Mango\": 5}\n",
    "child_node_nd = {\"Platano\": 400, \"Mango\": 5}\n",
    "child_node_rd = {\"Platano\": 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrada: {'Manzana': 100, 'Platano': 400, 'Mango': 5, 'Uva': 550}\n",
      "Suma Total: 1055\n",
      "Prob Manzana: 0.0947867298578199\n",
      "Prob Platano: 0.3791469194312796\n",
      "Prob Mango: 0.004739336492890996\n",
      "Prob Uva: 0.5213270142180095\n",
      "\n",
      "Suma de k probabilidades: 1.0\n",
      "Diccionario:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Manzana': 0.0947867298578199,\n",
       " 'Platano': 0.3791469194312796,\n",
       " 'Mango': 0.004739336492890996,\n",
       " 'Uva': 0.5213270142180095}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejercicio Dummy.\n",
    "print(\"Datos de entrada:\", Root_node)\n",
    "print(\"Suma Total:\", 100+400+5+550)\n",
    "print(\"Prob Manzana:\", 100/1055)\n",
    "print(\"Prob Platano:\", 400/1055)\n",
    "print(\"Prob Mango:\", 5/1055)\n",
    "print(\"Prob Uva:\", 550/1055)\n",
    "\n",
    "# Codifiquemos...\n",
    "node_sum = sum(Root_node.values())\n",
    "dict_prob = {c:v / node_sum for c, v in Root_node.items()}\n",
    "print(\"\\nSuma de k probabilidades:\", sum(dict_prob.values()))\n",
    "print(\"Diccionario:\")\n",
    "dict_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_class(node):\n",
    "    node_sum = sum(node.values())\n",
    "    percents = {c:v / node_sum for c, v in node.items()}\n",
    "    return node_sum, percents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini es la medida que menos costo computacional requiere por las operaciones aritméticas que realiza la función (la suma de las probabilidades al cuadrado). Por ende, a las probabilidades menores casi no las toma en cuenta, y por ende tienden a influir muy poco (o casi nada) en el resultado final.\n",
    "\n",
    "### _Cuenta demostrativa:_\n",
    "#### $0.0948^{2} + 0.3791^{2} + 0.0047^{2} + 0.5213^{2}$\n",
    "#### $0.009 + 0.1438 + 0.00002209 + 0.2718$\n",
    "#### $P = 0.4246$\n",
    "#### $Gini = 1 - P = 0.5754$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La función _gini score_ es una función que nos ayudará a obtener la magnitud Gini (explicada al inicio del notebook), para ello debemos crear por agregación al método, la función _probability class_ para mejorar y atomizar ambos comportamientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_score(node):\n",
    "    _, percents = probability_class(node)\n",
    "    # donde i contiene la probabilidad calculada del nodo en cuestión, con def \"probability_class\".\n",
    "    score = round(1 - sum([i**2 for i in percents.values()]), 3)\n",
    "    print('Gini Score for node {} : {}'.format(node, score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La función de Entropía es menos eficiente en términos computacionales por lo que realiza el algoritmo de $\\log_{2}$, pero lo interesante aquí es que como usa el $\\log$, a las probabilidades menores y mayores las ajusta y toma por *\"equivalente\"* aquellas probabilidades más pequeñas. Por lo que el resultado será mucho más sensible a dichas probabilidades, en los casos que se busque darle la misma relevancia a las probilidades más bajas, se recomienda usar la Entropía como medida de impureza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $E(S) = \\sum_{i = 1}^{c} - p_{i} \\log_{2} p_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Cuenta demostrativa:_\n",
    "#### $- 0.0948 \\log_{2}(0.0948) + -0.3791 \\log_{2}(0.3791) + -0.0047 \\log_{2}(0.0047) + -0.5213 \\log_{2}(0.5213)$\n",
    "#### $0.3222 + 0.5304 + 0.0363 + 0.4899$\n",
    "#### $P = 1.3788$\n",
    "#### $Gini = 1 - P = -0.3788$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_score(node):\n",
    "    _, percents = probability_class(node)\n",
    "    # donde i contiene la probabilidad calculada del nodo en cuestión, con def \"probability_class\".\n",
    "    score = round(sum([-i * log(i, 2) for i in percents.values()]), 3)\n",
    "    print('Entropy Score for node {} : {}'.format(node, score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculamos el information Gain, a menor impureza, mayor information Gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(parent, children, criterion):\n",
    "    score = {'gini': gini_score, 'entropy': entropy_score}\n",
    "    metric = score[criterion]\n",
    "    parent_score = metric(parent)\n",
    "    parent_sum = sum(parent.values())\n",
    "    weighted_child_score = sum([metric(i) * sum(i.values()) / parent_sum  for i in children])\n",
    "    gain = round((parent_score - weighted_child_score),2)\n",
    "    print('Information gain: {}'.format(gain))\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquí se muestra el funcionamiento de cada función tanto gini como entropía, se podrá distinguir que si, intencionalmente, incluímos la categoría \"Mango\". El número 5 quiere decir que estamos incluyendo solamente 5 registros que contienen la clase \"Mango\", por lo tanto podemos notar que hay un importante desbalanceo entre las cantidades por cada clase, por ejemplo:\n",
    "- En el caso de Mazana, Platano y Uva, éstas contienen cantidades por encima de los 100 registros, claramente son las clases predominantes en este ejemplo, por lo que serán de gran influencia en la separación de la información en cada nodo hijo\n",
    "- En el caso de la clase Mango, podemos asumir que la probabilidad calculada para ella será muy pequeña, por lo que será muy probable que la función de magnitud Gini ignore dicha clase\n",
    "- Por otro lado vemos que la función de Entropia, no solo logra distinguir a la clase minoritaria, sino que separa a mayor precisión la información que reside en cada clase dando como resultado final un mayor *Information Gain*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Score for node {'Manzana': 100, 'Platano': 400, 'Mango': 5, 'Uva': 550} : 0.575\n",
      "Gini Score for node {'Manzana': 100, 'Platano': 400, 'Mango': 5} : 0.333\n",
      "Gini Score for node {'Platano': 400, 'Mango': 5} : 0.024\n",
      "Gini Score for node {'Platano': 400} : 0.0\n",
      "Information gain: 0.41\n"
     ]
    }
   ],
   "source": [
    "gini_gain = information_gain(parent=Root_node, children=[child_node_st, child_node_nd, child_node_rd], criterion='gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy Score for node {'Manzana': 100, 'Platano': 400, 'Mango': 5, 'Uva': 550} : 1.379\n",
      "Entropy Score for node {'Manzana': 100, 'Platano': 400, 'Mango': 5} : 0.795\n",
      "Entropy Score for node {'Platano': 400, 'Mango': 5} : 0.096\n",
      "Entropy Score for node {'Platano': 400} : 0.0\n",
      "Information gain: 0.96\n"
     ]
    }
   ],
   "source": [
    "entropy_gain = information_gain(parent=Root_node, children=[child_node_st, child_node_nd, child_node_rd], criterion='entropy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
